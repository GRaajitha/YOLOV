{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f6585-095d-48ac-8296-72b0b7a3f3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix json if it includes missing images\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "val_file_path = \"\"\n",
    "output_path = \"\"\n",
    "orig_val_data = json.load(open(val_file_path))\n",
    "img_name_to_id = {img['file_name']:img['id'] for img in orig_val_data['images']}\n",
    "\n",
    "IMAGES_DIR = \"/shared/vision/dataset/\"\n",
    "def get_missing_images(images):\n",
    "    list_imgs_paths = [img['file_name'] for img in images]\n",
    "    missing_image_ids = []\n",
    "    for file_name in tqdm(list_imgs_paths):\n",
    "        if not os.path.exists(os.path.join(IMAGES_DIR, file_name)):\n",
    "            missing_image_ids.append(img_name_to_id[file_name])\n",
    "    return missing_image_ids\n",
    "\n",
    "missing_image_ids = get_missing_images(orig_val_data['images'])\n",
    "print(len(missing_image_ids))\n",
    "\n",
    "if len(missing_image_ids) > 0:\n",
    "    dataset = {}\n",
    "    dataset[\"categories\"] = orig_val_data[\"categories\"]\n",
    "    dataset[\"images\"] = [img for img in orig_val_data['images'] if img['id'] not in missing_image_ids]\n",
    "    dataset[\"annotations\"] = [annot for annot in orig_val_data['annotations'] if annot['image_id'] not in missing_image_ids]\n",
    "\n",
    "    with open(output_path, \"w\") as fp:\n",
    "        json.dump(dataset, fp, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce421936-1975-4dbd-b8b1-378d0b67e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize intermediate dataset\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "train_file = \"\"\n",
    "train_data = json.load(open(train_file, \"r\"))\n",
    "\n",
    "vid_id_to_anns = defaultdict(list)\n",
    "for ann in train_data[\"annotations\"]:\n",
    "    vid_id_to_anns[ann[\"video_id\"]].append(ann)\n",
    "\n",
    "viz_vids = random.sample(train_data['videos'], 10)\n",
    "\n",
    "for vid in viz_vids:\n",
    "    annotations = vid_id_to_anns[vid[\"id\"]]\n",
    "    file_path = vid['file_names'][0]\n",
    "    img_cv = cv2.imread(f\"{file_path}\")\n",
    "    # annotations = img_id_gts[img_id]\n",
    "    vid = vid[\"id\"]\n",
    "    for ann in annotations:\n",
    "        x1, y1, w, h = ann[\"bboxes\"][0]\n",
    "        xmin = int(x1)\n",
    "        ymin = int(y1)\n",
    "        xmax = int(x1+w)\n",
    "        ymax = int(y1+h)\n",
    "        img_cv = cv2.rectangle(img_cv, (xmin, ymin), (xmax, ymax), (0,255,0), 2)\n",
    "    cv2.imwrite(f\"/home/rgummadi/DetectAndAvoid/flywheel/scripts/OVIS_VIZ/{vid}.png\", img_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3a420-0b15-4ad3-8510-fe96d52d1de8",
   "metadata": {},
   "source": [
    "## visualize 10 images from coco dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abf9171-3aa9-426e-bf40-905154726ae8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize input to yolov++ \n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "data_dir = \"\"\n",
    "train_file = \"\"\n",
    "train_data = json.load(open(train_file, \"r\"))\n",
    "\n",
    "gt_img_ids_to_img = {img[\"id\"]:img[\"file_name\"] for img in train_data[\"images\"]}\n",
    "img_id_gts = defaultdict(list)\n",
    "for ann in train_data[\"annotations\"]:\n",
    "    img_id_gts[ann[\"image_id\"]].append(ann)\n",
    "    \n",
    "import random\n",
    "viz_img_ids = random.sample(list(gt_img_ids_to_img.keys()), 10)\n",
    "\n",
    "import cv2\n",
    "for img_id in viz_img_ids:\n",
    "    file_path = gt_img_ids_to_img[img_id]\n",
    "    img_cv = cv2.imread(f\"{data_dir}/{file_path}\")\n",
    "    annotations = img_id_gts[img_id]\n",
    "    for ann in annotations:\n",
    "        x1, y1, w, h = ann[\"bbox\"]\n",
    "        xmin = int(x1)\n",
    "        ymin = int(y1)\n",
    "        xmax = int(x1+w)\n",
    "        ymax = int(y1+h)\n",
    "        img_cv = cv2.rectangle(img_cv, (xmin, ymin), (xmax, ymax), (0,255,0), 1)\n",
    "    cv2.imwrite(f\"/home/rgummadi/DetectAndAvoid/flywheel/scripts/OVIS_VIZ/{img_id}.png\", img_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9feaa10-85fb-4bd8-b710-11c617a7d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize inputs to yolov++ evaluator\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def preproc(img, input_size, swap=(2, 0, 1)):\n",
    "    padded_img = np.ones((input_size[0], input_size[1], 3), dtype=np.uint8) * 114\n",
    "\n",
    "    r = min(input_size[0] / img.shape[0], input_size[1] / img.shape[1])\n",
    "    resized_img = cv2.resize(\n",
    "        img,\n",
    "        (int(img.shape[1] * r), int(img.shape[0] * r)),\n",
    "        interpolation=cv2.INTER_LINEAR,\n",
    "    ).astype(np.uint8)\n",
    "    padded_img[: int(img.shape[0] * r), : int(img.shape[1] * r)] = resized_img\n",
    "\n",
    "    padded_img = np.ascontiguousarray(padded_img, dtype=np.uint8)\n",
    "    return padded_img, r\n",
    "\n",
    "gt_file = \"\"\n",
    "pred_file = \"\"\n",
    "\n",
    "gt = json.load(open(gt_file, \"r\"))\n",
    "dt = json.load(open(pred_file, \"r\"))\n",
    "\n",
    "img_id_dets = defaultdict(list)\n",
    "for det in dt:\n",
    "    img_id_dets[det[\"image_id\"]].append(det)\n",
    "    \n",
    "img_id_gts = defaultdict(list)\n",
    "for ann in gt[\"annotations\"]:\n",
    "    img_id_gts[ann[\"image_id\"]].append(ann)\n",
    "    \n",
    "gt_img_ids_to_img = {ann[\"image_id\"]:ann[\"image_name\"] for ann in gt[\"annotations\"]}\n",
    "viz_img_ids = random.sample(list(gt_img_ids_to_img.keys()), min(10, len(gt_img_ids_to_img.keys())))\n",
    "\n",
    "input_size = (1920, 1920)\n",
    "for img_id in viz_img_ids:\n",
    "    file_path = gt_img_ids_to_img[img_id]\n",
    "    img_cv = cv2.imread(file_path)\n",
    "    # _, r = preproc(img_cv, input_size)\n",
    "    detections = img_id_dets[img_id]\n",
    "    annotations = img_id_gts[img_id]\n",
    "    for det in detections:\n",
    "        if det['score'] > 0.1:\n",
    "            xmin, ymin, w, h = map(int, [val for val in det[\"bbox\"]])\n",
    "            xmax = xmin+w\n",
    "            ymax = ymin+h\n",
    "            img_cv = cv2.rectangle(img_cv, (xmin, ymin), (xmax, ymax), (0,0,255), 2)\n",
    "            img_cv = cv2.putText(img_cv, str(det['category_id']),(xmin-5, ymin-5), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "    for ann in annotations:\n",
    "        xmin, ymin, w, h = map(int, [val for val in ann[\"bbox\"]])\n",
    "        xmax = xmin+w\n",
    "        ymax = ymin+h\n",
    "        img_cv = cv2.rectangle(img_cv, (xmin, ymin), (xmax, ymax), (0,255,0), 2)\n",
    "        img_cv = cv2.putText(img_cv, str(det['category_id']),(xmax+5, ymax+5), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "    cv2.imwrite(f\"/home/rgummadi/DetectAndAvoid/flywheel/scripts/OVIS_VIZ/{img_id}.png\", img_cv)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02d4ff39-f9aa-44be-bd2e-074776333a63",
   "metadata": {},
   "source": [
    "expected format\n",
    "vid_image = {\"license\": None, \"file_name\": \"ILSVRC/Data/DET/train/ILSVRC2013_train/n03770679/n03770679_10598.JPEG\", \"coco_url\": None, \"height\": 375, \"width\": 500, \"date_captured\": None, \"flickr_url\": None, \"id\": 1}\n",
    "print(\"vid_image\" , vid_image)\n",
    "vid_categories = { \"supercategorie\": \"\", \"id\": 0, \"name\": \"airplane\"}\n",
    "print(\"categories\", vid_categories)\n",
    "vid_ann = {\"segmentations\": [], \"area\": 30340, \"iscrowd\": 0, \"image_id\": 23, \"bbox\": [162, 72, 148, 205], \"category_id\": 4, \"id\": 28}\n",
    "print(\"vid_ann\", vid_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32783f-4b7e-4511-8295-2686f652e819",
   "metadata": {},
   "source": [
    "## Convert zipline coco dataset to coco-video dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33039226-584f-43e5-838c-c2e0beac43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert zipline dataset to coco-video dataset format\n",
    "\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "v7_file = \"/shared/vision/dataset/metadata/v7_8_cls/test_annotations_coco_fmt.json\"\n",
    "Video_name_idx = 3 if \"zeromatter\" in v7_file else 1\n",
    "data_dir = \"/shared/vision/dataset/\"\n",
    "new_json_file = \"/shared/vision/dataset/metadata/v7_8_cls/coco_vid/test_coco_vid_06_06.json\"\n",
    "v7_data = json.load(open(v7_file, \"r\"))\n",
    "# single_category = [{'supercategory': 'none', 'id': 0, 'name': 'Airborne'}]\n",
    "# categories = [{'supercategory': 'none', 'id': 0, 'name': 'Airborne'},\n",
    "#  {'supercategory': 'none', 'id': 1, 'name': 'Zip'},\n",
    "#  {'supercategory': 'none', 'id': 2, 'name': 'Glider'},\n",
    "#  {'supercategory': 'none', 'id': 3, 'name': 'Balloon'},\n",
    "#  {'supercategory': 'none', 'id': 4, 'name': 'Paraglider'},\n",
    "#  {'supercategory': 'none', 'id': 5, 'name': 'Bird'},\n",
    "#  {'supercategory': 'none', 'id': 6, 'name': 'Flock'},\n",
    "#  {'supercategory': 'none', 'id': 7, 'name': 'Airplane'},\n",
    "#  {'supercategory': 'none', 'id': 8, 'name': 'Ultralight'},\n",
    "#  {'supercategory': 'none', 'id': 9, 'name': 'Helicopter'},\n",
    "#  {'supercategory': 'none', 'id': 10, 'name': 'Unknown'},\n",
    "#  {'supercategory': 'none', 'id': 11, 'name': 'HangGlider'},\n",
    "#  {'supercategory': 'none', 'id': 12, 'name': 'CommercialAirliner'},\n",
    "#  {'supercategory': 'none', 'id': 13, 'name': 'Drone'},\n",
    "#  {'supercategory': 'none', 'id': 14, 'name': 'Artificial'},\n",
    "#  {'supercategory': 'none', 'id': 15, 'name': 'Natural'}]\n",
    "\n",
    "categories = [{'supercategory': 'none', 'id': 0, 'name': 'Airplane'},\n",
    " {'supercategory': 'none', 'id': 1, 'name': 'Paraglider'},\n",
    " {'supercategory': 'none', 'id': 2, 'name': 'Helicopter'},\n",
    " {'supercategory': 'none', 'id': 3, 'name': 'Zip'},\n",
    " {'supercategory': 'none', 'id': 4, 'name': 'Ultralight'},\n",
    " {'supercategory': 'none', 'id': 5, 'name': 'Glider'},\n",
    " {'supercategory': 'none', 'id': 6, 'name': 'Bird'},\n",
    " {'supercategory': 'none', 'id': 7, 'name': 'Balloon'}]\n",
    "\n",
    "# categories = [{'supercategory': 'none', 'id': 0, 'name': 'aircraft'},\n",
    "#               {'supercategory': 'none', 'id': 1, 'name': 'aircraft-nav-light'},\n",
    "#               {'supercategory': 'none', 'id': 2, 'name': 'strobe'}]\n",
    "\n",
    "vid_to_images = defaultdict(list)\n",
    "img_id_to_vid = {}\n",
    "unique_videos = set()\n",
    "\n",
    "videos = []\n",
    "# categories = []\n",
    "images = []\n",
    "\n",
    "vid = 0\n",
    "img_id = 0\n",
    "old_img_id_to_new_img_id = {}\n",
    "# subsampled_img_ids = [img[\"id\"] for img in v7_data[\"images\"]]\n",
    "for image in tqdm(v7_data[\"images\"]):\n",
    "    vid_num = image[\"file_name\"].split(\"/\")[Video_name_idx]\n",
    "    image_file_name = os.path.join(data_dir, image[\"file_name\"])\n",
    "    vid_to_images[vid_num].append(image)\n",
    "    old_img_id_to_new_img_id[image[\"id\"]] = img_id\n",
    "    new_image = {\"id\": img_id,\n",
    "                 \"width\": image[\"width\"], \n",
    "                 \"height\": image[\"height\"],\n",
    "                 \"name\": image_file_name}\n",
    "    if vid_num in unique_videos:\n",
    "        videos[vid-1][\"length\"] += 1\n",
    "        videos[vid-1][\"file_names\"].append(image_file_name)\n",
    "        img_id_to_vid[img_id]=vid-1\n",
    "        new_image[\"sid\"] = vid-1\n",
    "        new_image[\"fid\"] = video[\"length\"]-1\n",
    "    else:\n",
    "        video = {\"id\": vid, \"width\": image[\"width\"], \"height\": image[\"height\"], \"length\": 1, \"file_names\": [image_file_name]}\n",
    "        videos.append(video)\n",
    "        img_id_to_vid[img_id]=vid\n",
    "        new_image[\"sid\"] = vid\n",
    "        new_image[\"fid\"] = video[\"length\"]-1\n",
    "        vid+=1\n",
    "        unique_videos.add(vid_num)\n",
    "    images.append(new_image)\n",
    "    img_id += 1\n",
    "    \n",
    "annotations = []\n",
    "ann_id = 0\n",
    "for ann in tqdm(v7_data[\"annotations\"]):\n",
    "    # if ann[\"image_id\"] not in subsampled_img_ids:\n",
    "    #     continue\n",
    "    annotation = {\"id\": ann_id,\n",
    "                 \"category_id\": ann[\"category_id\"],\n",
    "                 \"sid\": img_id_to_vid[ old_img_id_to_new_img_id[ann[\"image_id\"]]],\n",
    "                 \"iscrowd\": False, \n",
    "                 \"ignore\": False,\n",
    "                 \"bbox\": ann[\"bbox\"],\n",
    "                 \"area\": ann[\"area\"],\n",
    "                 \"image_id\": old_img_id_to_new_img_id[ann[\"image_id\"]],\n",
    "                 'attributes': ann[\"attributes\"]}\n",
    "    annotations.append(annotation)\n",
    "    ann_id+=1\n",
    "\n",
    "# categories = sorted(v7_data[\"categories\"], key=lambda x: x[\"id\"])\n",
    "new_json_data = {\"info\": {}, \"categories\": categories, \"videos\": videos, \"images\": images, \"annotations\": annotations}\n",
    "# new_json_data = {\"info\": {}, \"categories\": single_category, \"videos\": videos, \"images\": images, \"annotations\": annotations}\n",
    "with open(new_json_file, \"w\") as f:\n",
    "    json.dump(new_json_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66495a8-f1b4-4968-895f-110ad4ac3b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"/shared/vision/dataset/metadata/v7_8_cls/coco_vid/test_coco_vid_06_06.json\", \"r\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df90aa-d122-44ca-a0c7-c806faf2e909",
   "metadata": {},
   "source": [
    "## Split videos if filenames skip by more than 10 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c6fb2-f933-405c-bc25-01cd7ad9e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "\n",
    "data = json.load(open(\"/shared/vision/dataset/metadata/v7_8_cls/coco_vid/val_coco_vid_06_06.json\", \"r\"))\n",
    "new_json_outfile = \"/shared/vision/dataset/metadata/v7_8_cls/coco_vid/val_coco_vid_06_06_split_vid.json\"\n",
    "\n",
    "stride = 10 # when there's >=10 frames gap in between a single video, split to new video\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "video_to_image = defaultdict(list) # old_video_id: [list of images that belong to it?]\n",
    "for image in data[\"images\"]:\n",
    "    old_video_id = image[\"sid\"]\n",
    "    video_to_image[old_video_id].append(image)\n",
    "    \n",
    "video_to_annot = defaultdict(list) # old_video_id: [list of images that belong to it?]\n",
    "for annot in data[\"annotations\"]:\n",
    "    old_video_id = annot[\"sid\"]\n",
    "    video_to_annot[old_video_id].append(annot)\n",
    "    \n",
    "image_filename_to_image_entries = defaultdict(dict) # old_video_id: [list of images that belong to it?]\n",
    "for image in data[\"images\"]:    \n",
    "    image_filename_to_image_entries[image[\"name\"]] = image\n",
    "    \n",
    "image_id_to_annot_entry = defaultdict(list) # old_video_id: [list of images that belong to it?]\n",
    "for annot in data[\"annotations\"]:    \n",
    "    image_id_to_annot_entry[annot[\"image_id\"]].append(annot)\n",
    "\n",
    "########################################\n",
    "# Initialize new json fields\n",
    "new_info = data['info']\n",
    "new_categories = data['categories']\n",
    "new_videos = []\n",
    "new_images = []\n",
    "new_annotations = []\n",
    "########################################\n",
    "\n",
    "running_video_id_counter = 0\n",
    "for big_video_entry in data[\"videos\"]:\n",
    "    current_chunked_video_entry = copy.deepcopy(big_video_entry)\n",
    "    current_chunked_video_entry['id'] = running_video_id_counter\n",
    "    current_chunked_video_entry['length'] = 0          # TODO this must be overwritten by the end\n",
    "    current_chunked_video_entry['file_names'] = []\n",
    "\n",
    "\n",
    "    last_image_id = -1\n",
    "    for filename in big_video_entry[\"file_names\"]:\n",
    "        image_id = int(filename.split('/')[-1].split('.')[0])\n",
    "        \n",
    "        # Use this filename to create a new video entry\n",
    "        if (last_image_id != -1) and (image_id - last_image_id >= stride):\n",
    "            \n",
    "            images_for_old_video_id = video_to_image[big_video_entry['id']]\n",
    "            \n",
    "            # go thru the filenames we have in this video's segment, and pluck them from the\n",
    "            \n",
    "            image_fid_counter = 0\n",
    "            for image_filename in current_chunked_video_entry['file_names']:\n",
    "                image_entry = copy.deepcopy(image_filename_to_image_entries[image_filename])\n",
    "                \n",
    "                ## updating sid\n",
    "                image_entry[\"sid\"] = running_video_id_counter\n",
    "                ## updating fid\n",
    "                image_entry['fid'] = image_fid_counter\n",
    "                image_fid_counter+=1   \n",
    "                new_images.append(image_entry)\n",
    "\n",
    "                #### DO ANNOTS HERE ####\n",
    "                # for annots, only sid id has to be updated to match new video id\n",
    "                # for image id, should pull all annots for that image id...\n",
    "                # and put those into list of annots for the video\n",
    "\n",
    "                annots_for_this_image = image_id_to_annot_entry[image_entry[\"id\"]]                \n",
    "                for annot in annots_for_this_image:\n",
    "                    new_annot = copy.deepcopy(annot)\n",
    "                    new_annot[\"sid\"] = running_video_id_counter\n",
    "                    new_annotations.append(new_annot)\n",
    "                \n",
    "            # prepare to add the completed video dict to the list of video dicts, and reset all fields to begin on\n",
    "            # this next one... do all increments, and continue\n",
    "            video_frame_length = len(current_chunked_video_entry['file_names'])\n",
    "            current_chunked_video_entry['length'] = video_frame_length\n",
    "            \n",
    "            # Increment, to prepare id for next video sequence\n",
    "            running_video_id_counter+=1\n",
    "            new_videos.append(copy.deepcopy(current_chunked_video_entry))\n",
    "            \n",
    "            # setting up new video dict\n",
    "            current_chunked_video_entry['id'] = running_video_id_counter\n",
    "            current_chunked_video_entry['length'] = 0\n",
    "            current_chunked_video_entry['file_names'] = [filename] ## add current filename, to kick it off\n",
    "            \n",
    "            ## the current video id is running_video_id_counter...\n",
    "            ## print(f\"running_video_id_counter: {running_video_id_counter}\")\n",
    "        \n",
    "        # Add this filename to current video entry + continue\n",
    "        else:\n",
    "            # print(\"no need to create new\")\n",
    "            current_chunked_video_entry['file_names'].append(filename)\n",
    "            ## print(current_chunked_video_entry)\n",
    "            \n",
    "        last_image_id = image_id\n",
    "        \n",
    "        \n",
    "#     print(\"_______CREATING_NEW_VIDEO______\")\n",
    "    # Now we finished processing that video... pack up and add to new_videos, this one, before moving to next entry\n",
    "    # prepare to add the completed video dict to the list of video dicts, and reset all fields to begin on\n",
    "    # this next one... do all increments, and continue\n",
    "    video_frame_length = len(current_chunked_video_entry['file_names'])\n",
    "    current_chunked_video_entry['length'] = video_frame_length\n",
    "    \n",
    "    \n",
    "    # lets process all images and annotations for this video id, and update the \"sid\"s for them, as well\n",
    "    # as the \"fid\" and place in the final dicts\n",
    "    images_for_old_video_id = video_to_image[old_video_id]\n",
    "\n",
    "\n",
    "    # go thru the filenames we have in this video's segment, and pluck them from the \n",
    "    image_fid_counter = 0\n",
    "    for image_filename in current_chunked_video_entry['file_names']:\n",
    "        image_entry = copy.deepcopy(image_filename_to_image_entries[image_filename])\n",
    "\n",
    "        ## updating sid\n",
    "        image_entry[\"sid\"] = running_video_id_counter\n",
    "        ## updating fid\n",
    "        image_entry['fid'] = image_fid_counter\n",
    "        image_fid_counter+=1\n",
    "\n",
    "        new_images.append(image_entry)            \n",
    "        #### DO ANNOTS HERE ####\n",
    "        # for annots, only sid id has to be updated to match new video id\n",
    "        # for image id, should pull all annots for that image id...\n",
    "        # and put those into list of annots for the video\n",
    "\n",
    "        annots_for_this_image = image_id_to_annot_entry[image_entry[\"id\"]]\n",
    "        for annot in annots_for_this_image:\n",
    "            new_annot = copy.deepcopy(annot)\n",
    "            new_annot[\"sid\"] = running_video_id_counter\n",
    "            new_annotations.append(new_annot)\n",
    "\n",
    "    # Increment, to prepare id for next video sequence\n",
    "    running_video_id_counter+=1\n",
    "    new_videos.append(copy.deepcopy(current_chunked_video_entry))\n",
    "    \n",
    "print(new_info)\n",
    "print(new_categories)\n",
    "print(len(new_videos))\n",
    "print(len(new_images))\n",
    "print(len(new_annotations))\n",
    "\n",
    "new_json_data = {\"info\": new_info, \"categories\": new_categories, \"videos\": new_videos, \"images\": new_images, \"annotations\": new_annotations}\n",
    "\n",
    "with open(new_json_outfile, \"w\") as f:\n",
    "    json.dump(new_json_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e21a22-9938-47b2-b030-012220cecde7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open(\"\", \"r\"))\n",
    "print(\"no of videos\", len(data[\"videos\"]))\n",
    "print(\"no of images\", len(data[\"images\"]))\n",
    "print(\"no of annotations\", len(data[\"annotations\"]))\n",
    "for ann in data[\"annotations\"]:\n",
    "    if type(ann[\"category_id\"]) == list:\n",
    "        ann[\"category_id\"] = ann[\"category_id\"][0]\n",
    "    \n",
    "with open(\"\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19476f51-e867-49fd-9af9-5691c802cb02",
   "metadata": {},
   "source": [
    "## Trim coco ovis data format to fixed len number of images per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401eba7-bc11-483b-8473-3daa58c18354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim coco ovis data format to fixed len number of images per video\n",
    "\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "data = json.load(open(\"/shared/vision/dataset/metadata/v7_8_cls/coco_vid/val_coco_vid_06_06.json\", \"r\"))\n",
    "print(\"videos: \", len(data[\"videos\"]), \" images: \", len(data[\"images\"]), \" annoatations: \", len(data[\"annotations\"]))\n",
    "\n",
    "videos = []\n",
    "old_vid_id_to_new_vid_id = {}\n",
    "new_v_id = 0\n",
    "skipped_videos = []\n",
    "min_seq_len = 64\n",
    "seq_len = 500\n",
    "trimmed_file = f\"/shared/vision/dataset/metadata/v7_8_cls/coco_vid/trimmed1000_64-500seq_val_coco_vid_06_06.json\"\n",
    "max_videos = 100000\n",
    "\n",
    "for vid in tqdm(data[\"videos\"]):\n",
    "    if new_v_id >= max_videos:\n",
    "        skipped_videos.append(vid[\"id\"])\n",
    "        continue\n",
    "    if vid[\"length\"] < min_seq_len:\n",
    "        skipped_videos.append(vid[\"id\"])\n",
    "        continue\n",
    "    else:\n",
    "        old_vid_id_to_new_vid_id[vid[\"id\"]] = new_v_id\n",
    "        vid[\"file_names\"] = vid[\"file_names\"][:seq_len]\n",
    "        vid[\"length\"] = seq_len\n",
    "        vid[\"id\"] = new_v_id\n",
    "        videos.append(vid)\n",
    "        new_v_id += 1\n",
    "\n",
    "images = [] \n",
    "new_img_id = 0\n",
    "old_img_id_to_new_img_id = {}\n",
    "skipped_images = []\n",
    "for img in tqdm(data[\"images\"]):\n",
    "    if img[\"sid\"] in skipped_videos:\n",
    "        skipped_images.append(img[\"id\"])\n",
    "        continue\n",
    "        \n",
    "    if img[\"fid\"] >= seq_len:\n",
    "        skipped_images.append(img[\"id\"])\n",
    "        continue\n",
    "    old_img_id_to_new_img_id[img[\"id\"]]=new_img_id\n",
    "    img[\"id\"] = new_img_id\n",
    "    img[\"sid\"] = old_vid_id_to_new_vid_id[img[\"sid\"]]\n",
    "    images.append(img)\n",
    "    new_img_id += 1\n",
    "\n",
    "new_ann_id = 0\n",
    "annotations = []\n",
    "for ann in tqdm(data[\"annotations\"]):\n",
    "    if ann[\"sid\"] in skipped_videos:\n",
    "        continue\n",
    "    if ann[\"image_id\"] in skipped_images:\n",
    "        continue\n",
    "    ann[\"id\"] = new_ann_id\n",
    "    ann[\"sid\"] = old_vid_id_to_new_vid_id[ann[\"sid\"]]\n",
    "    ann[\"image_id\"] = old_img_id_to_new_img_id[ann[\"image_id\"]]\n",
    "    ann[\"category_id\"] = ann[\"category_id\"][0] if type(ann[\"category_id\"])==list else ann[\"category_id\"]\n",
    "    annotations.append(ann)\n",
    "    new_ann_id += 1\n",
    "\n",
    "categories = [{'supercategory': 'none', 'id': 0, 'name': 'Airplane'},\n",
    " {'supercategory': 'none', 'id': 1, 'name': 'Paraglider'},\n",
    " {'supercategory': 'none', 'id': 2, 'name': 'Helicopter'},\n",
    " {'supercategory': 'none', 'id': 3, 'name': 'Zip'},\n",
    " {'supercategory': 'none', 'id': 4, 'name': 'Ultralight'},\n",
    " {'supercategory': 'none', 'id': 5, 'name': 'Glider'},\n",
    " {'supercategory': 'none', 'id': 6, 'name': 'Bird'},\n",
    " {'supercategory': 'none', 'id': 7, 'name': 'Balloon'}]\n",
    "trimmed_data = {\"info\": {}, \"categories\": categories, \"videos\": videos, \"images\": images, \"annotations\": annotations}\n",
    "with open(trimmed_file, \"w\") as f:\n",
    "    json.dump(trimmed_data, f, indent=4)\n",
    "\n",
    "print(\"no of videos\", len(trimmed_data[\"videos\"]))\n",
    "print(\"no of images\", len(trimmed_data[\"images\"]))\n",
    "print(\"no of annotations\", len(trimmed_data[\"annotations\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be386129-8fc9-4a3a-b2a4-9845e252e685",
   "metadata": {},
   "source": [
    "## Run sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fac0d4-fb03-4b91-b5cf-21492bc6263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e2098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open(\"\", \"r\"))\n",
    "ann_ids = [ann[\"id\"] for ann in data[\"annotations\"]]\n",
    "assert len(set(ann_ids))==len(ann_ids)\n",
    "\n",
    "img_ids = [img[\"id\"] for img in data[\"images\"]]\n",
    "ann_img_ids_not_in_imgs = [ann[\"image_id\"] for ann in data[\"annotations\"] if ann[\"image_id\"] not in img_ids]\n",
    "print(len(ann_img_ids_not_in_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae9efc7-e468-40d3-907b-d33e02250efc",
   "metadata": {},
   "source": [
    "## visualize 10 images from coco vid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe9766-178a-4004-8b97-8bf1d370542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize input to yolov++ \n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "data_dir = \"\"\n",
    "train_file = \"\"\n",
    "train_data = json.load(open(train_file, \"r\"))\n",
    "\n",
    "gt_img_ids_to_img = {img[\"id\"]:img[\"name\"] for img in train_data[\"images\"]}\n",
    "img_id_gts = defaultdict(list)\n",
    "for ann in train_data[\"annotations\"]:\n",
    "    img_id_gts[ann[\"image_id\"]].append(ann)\n",
    "    \n",
    "import random\n",
    "viz_img_ids = random.sample(list(gt_img_ids_to_img.keys()), 10)\n",
    "\n",
    "import cv2\n",
    "for img_id in viz_img_ids:\n",
    "    file_path = gt_img_ids_to_img[img_id]\n",
    "    img_cv = cv2.imread(file_path)\n",
    "    annotations = img_id_gts[img_id]\n",
    "    for ann in annotations:\n",
    "        x1, y1, w, h = ann[\"bbox\"]\n",
    "        xmin = int(x1)\n",
    "        ymin = int(y1)\n",
    "        xmax = int(x1+w)\n",
    "        ymax = int(y1+h)\n",
    "        img_cv = cv2.rectangle(img_cv, (xmin, ymin), (xmax, ymax), (0,255,0), 1)\n",
    "    cv2.imwrite(f\"/home/rgummadi/DetectAndAvoid/flywheel/scripts/OVIS_VIZ/{img_id}.png\", img_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d632c08f-0407-4b5b-a57f-88bae745fbc0",
   "metadata": {},
   "source": [
    "## Split coco vid dataset into train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a670021-1680-46df-a111-495d64ae711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize input to yolov++ \n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "data_dir = \"\"\n",
    "train_file = \"\"\n",
    "train_data = json.load(open(train_file, \"r\"))\n",
    "\n",
    "gt_img_ids_to_img = {img[\"id\"]:img[\"name\"] for img in train_data[\"images\"]}\n",
    "img_id_gts = defaultdict(list)\n",
    "for ann in train_data[\"annotations\"]:\n",
    "    img_id_gts[ann[\"image_id\"]].append(ann)\n",
    "    \n",
    "import random\n",
    "viz_img_ids = random.sample(list(gt_img_ids_to_img.keys()), 10)\n",
    "\n",
    "import cv2\n",
    "for img_id in viz_img_ids:\n",
    "    file_path = gt_img_ids_to_img[img_id]\n",
    "    img_cv = cv2.imread(file_path)\n",
    "    annotations = img_id_gts[img_id]\n",
    "    for ann in annotations:\n",
    "        x1, y1, w, h = ann[\"bbox\"]\n",
    "        xmin = int(x1)\n",
    "        ymin = int(y1)\n",
    "        xmax = int(x1+w)\n",
    "        ymax = int(y1+h)\n",
    "        img_cv = cv2.rectangle(img_cv, (xmin, ymin), (xmax, ymax), (0,255,0), 1)\n",
    "    cv2.imwrite(f\"/home/rgummadi/DetectAndAvoid/flywheel/scripts/OVIS_VIZ/{img_id}.png\", img_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d17ee66",
   "metadata": {},
   "source": [
    "## Split coco vid dataset into train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "data = json.load(open(\"\", \"r\"))\n",
    "print(\"videos: \", len(data[\"videos\"]), \" images: \", len(data[\"images\"]), \" annoatations: \", len(data[\"annotations\"]))\n",
    "\n",
    "train_file = \"\"\n",
    "val_file = \"\"\n",
    "\n",
    "vids = [vid[\"id\"] for vid in data[\"videos\"]]\n",
    "train_vids = random.sample(vids, int(0.8*len(vids)))\n",
    "val_vids = [vid for vid in vids if vid not in train_vids]\n",
    "print(train_vids, val_vids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781d025-a6c0-45a9-abb3-56719275adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import copy\n",
    "\n",
    "random.seed(32)\n",
    "\n",
    "data = json.load(open(\"\", \"r\"))\n",
    "print(\"videos: \", len(data[\"videos\"]), \" images: \", len(data[\"images\"]), \" annoatations: \", len(data[\"annotations\"]))\n",
    "\n",
    "train_file = \"\"\n",
    "val_file = \"\"\n",
    "\n",
    "vids = [vid[\"id\"] for vid in data[\"videos\"]]\n",
    "train_vids = random.sample(vids, int(0.8*len(vids)))\n",
    "val_vids = [v_id for v_id in vids if v_id not in train_vids]\n",
    "print(train_vids, val_vids)\n",
    "\n",
    "def subsample_dataset(data, required_vids):\n",
    "    new_v_id = 0\n",
    "    videos = []\n",
    "    old_vid_id_to_new_vid_id = {}\n",
    "    skipped_videos = []\n",
    "    for vid in tqdm(data[\"videos\"]):\n",
    "        if vid[\"id\"] not in required_vids:\n",
    "            skipped_videos.append(vid[\"id\"])\n",
    "            continue\n",
    "        else:\n",
    "            old_vid_id_to_new_vid_id[vid[\"id\"]] = new_v_id\n",
    "            vid[\"id\"] = new_v_id\n",
    "            videos.append(vid)\n",
    "            new_v_id += 1\n",
    "\n",
    "    images = [] \n",
    "    new_img_id = 0\n",
    "    old_img_id_to_new_img_id = {}\n",
    "    skipped_images = []\n",
    "    for img in tqdm(data[\"images\"]):\n",
    "        if img[\"sid\"] in skipped_videos:\n",
    "            skipped_images.append(img[\"id\"])\n",
    "            continue\n",
    "\n",
    "        old_img_id_to_new_img_id[img[\"id\"]]=new_img_id\n",
    "        img[\"id\"] = new_img_id\n",
    "        img[\"sid\"] = old_vid_id_to_new_vid_id[img[\"sid\"]]\n",
    "        images.append(img)\n",
    "        new_img_id += 1\n",
    "\n",
    "    new_ann_id = 0\n",
    "    annotations = []\n",
    "    for ann in tqdm(data[\"annotations\"]):\n",
    "        if ann[\"sid\"] in skipped_videos:\n",
    "            continue\n",
    "        if ann[\"image_id\"] in skipped_images:\n",
    "            continue\n",
    "        ann[\"id\"] = new_ann_id\n",
    "        ann[\"sid\"] = old_vid_id_to_new_vid_id[ann[\"sid\"]]\n",
    "        ann[\"image_id\"] = old_img_id_to_new_img_id[ann[\"image_id\"]]\n",
    "        annotations.append(ann)\n",
    "        new_ann_id += 1\n",
    "\n",
    "    new_data = {\"info\": {}, \"categories\": data[\"categories\"], \"videos\": videos, \"images\": images, \"annotations\": annotations}\n",
    "\n",
    "    return new_data\n",
    "\n",
    "train_data = subsample_dataset(copy.deepcopy(data), train_vids)\n",
    "val_data = subsample_dataset(copy.deepcopy(data), val_vids)\n",
    "with open(train_file, \"w\") as f:\n",
    "    json.dump(train_data, f, indent=4)\n",
    "\n",
    "print(\"=== Training dataset ===\")\n",
    "print(\"no of videos\", len(train_data[\"videos\"]))\n",
    "print(\"no of images\", len(train_data[\"images\"]))\n",
    "print(\"no of annotations\", len(train_data[\"annotations\"]))\n",
    "with open(val_file, \"w\") as f:\n",
    "    json.dump(val_data, f, indent=4)\n",
    "\n",
    "print(\"=== Validation dataset ===\")\n",
    "print(\"no of videos\", len(val_data[\"videos\"]))\n",
    "print(\"no of images\", len(val_data[\"images\"]))\n",
    "print(\"no of annotations\", len(val_data[\"annotations\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dff52b-ccb6-4aad-b6be-822d5c1f4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a tiny coco json\n",
    "import json\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "data  = json.load(open(\"/shared/vision/dataset/metadata/v7_8_cls/test_annotations_coco_fmt.json\", \"r\"))\n",
    "tiny_dir = \"/shared/vision/dataset/metadata/v7_8_cls/tiny/\"\n",
    "tiny_file = os.path.join(tiny_dir, \"test_annotations_coco_fmt.json\")\n",
    "os.makedirs(tiny_dir, exist_ok=True)\n",
    "\n",
    "random.seed(42)\n",
    "subsampled_imgs = random.sample(data[\"images\"], 2)\n",
    "subsampled_img_ids = [img[\"id\"] for img in subsampled_imgs]\n",
    "old_ids_to_new_ids = {}\n",
    "new_img_id = 0\n",
    "images = []\n",
    "for img in (subsampled_imgs):\n",
    "    img_copy = deepcopy(img)\n",
    "    old_id = img_copy[\"id\"]\n",
    "    img_copy[\"id\"] = new_img_id\n",
    "    old_ids_to_new_ids[old_id] = new_img_id\n",
    "    images.append(img_copy)\n",
    "    new_img_id += 1\n",
    "\n",
    "new_ann_id = 0\n",
    "annotations = []\n",
    "for ann in tqdm(data[\"annotations\"]):\n",
    "    if ann[\"image_id\"] in subsampled_img_ids:\n",
    "        ann_copy = deepcopy(ann)\n",
    "        ann_copy[\"image_id\"] = old_ids_to_new_ids[ann[\"image_id\"]]\n",
    "        ann_copy[\"id\"] = new_ann_id\n",
    "        new_ann_id += 1\n",
    "        annotations.append(ann_copy)\n",
    "        \n",
    "tiny_data = {\"info\": {}, \"categories\": data[\"categories\"], \"images\": images, \"annotations\": annotations}\n",
    "\n",
    "with open(tiny_file, \"w\") as f:\n",
    "    json.dump(tiny_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f239a28c-bd95-4ec4-89a8-2b125cfa550d",
   "metadata": {},
   "source": [
    "# ZM-Synthetic logs processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a3fa7-6494-42f3-a3e3-ab1833ed05ac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Config\n",
    "zm_folder = \"/shared/vision/dataset/zeromatter_synthetic\"\n",
    "data_dir = \"/shared/vision/dataset/\"\n",
    "renders = [os.path.join(zm_folder, folder) for folder in os.listdir(zm_folder) if folder != \"depricated\"]\n",
    "train_ratio = 0.8\n",
    "output_train_path = os.path.join(zm_folder, \"coco_train_06_04_v2.json\")\n",
    "output_val_path = os.path.join(zm_folder, \"coco_val_06_04_v2.json\")\n",
    "\n",
    "def find_all_coco_annotations(base_dirs):\n",
    "    coco_files = []\n",
    "    for base_dir in base_dirs:\n",
    "        base_path = Path(base_dir)\n",
    "        coco_files.extend(base_path.rglob('coco_annotations.json'))\n",
    "    return [str(path.resolve()) for path in coco_files]\n",
    "\n",
    "all_coco_paths = find_all_coco_annotations(renders)\n",
    "\n",
    "# Initialize counters and data structures\n",
    "new_img_id = 0\n",
    "new_ann_id = 0\n",
    "video_to_images = defaultdict(list)\n",
    "image_id_to_annotations = defaultdict(list)\n",
    "\n",
    "# Load and remap annotations\n",
    "for coco_json in all_coco_paths:\n",
    "    with open(coco_json, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    img_id_to_ann = defaultdict(list)\n",
    "    for ann in data[\"annotations\"]:\n",
    "        img_id_to_ann[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "    img_id_to_images = {img[\"id\"]: img for img in data[\"images\"]}\n",
    "\n",
    "    for old_img_id, annotations in img_id_to_ann.items():\n",
    "        if not annotations:\n",
    "            continue\n",
    "\n",
    "        img = img_id_to_images[old_img_id]\n",
    "        img[\"id\"] = new_img_id\n",
    "        img[\"file_name\"] = img[\"file_name\"].replace(\"/systems/systems/\", \"/systems/\", 1)\n",
    "\n",
    "        # Derive a \"video name\" from the path\n",
    "        # This assumes the path includes the video at a fixed depth. Adjust if needed.\n",
    "        path_parts = Path(img[\"file_name\"]).parts\n",
    "        if \"systems\" in path_parts:\n",
    "            systems_idx = path_parts.index(\"systems\")\n",
    "            video_name = \"/\".join(path_parts[:systems_idx + 2])  # up to systems/[video]\n",
    "        else:\n",
    "            video_name = \"unknown\"\n",
    "\n",
    "        video_to_images[video_name].append(img)\n",
    "        for ann in annotations:\n",
    "            ann[\"image_id\"] = new_img_id\n",
    "            ann[\"id\"] = new_ann_id\n",
    "            image_id_to_annotations[new_img_id].append(ann)\n",
    "            new_ann_id += 1\n",
    "\n",
    "        new_img_id += 1\n",
    "\n",
    "# Split videos into train/val\n",
    "all_video_names = list(video_to_images.keys())\n",
    "random.shuffle(all_video_names)\n",
    "\n",
    "split_index = int(len(all_video_names) * train_ratio)\n",
    "train_video_names = set(all_video_names[:split_index])\n",
    "val_video_names = set(all_video_names[split_index:])\n",
    "\n",
    "# Collect images and annotations for train/val\n",
    "train_images, val_images = [], []\n",
    "train_annotations, val_annotations = [], []\n",
    "\n",
    "for video_name in train_video_names:\n",
    "    for img in video_to_images[video_name]:\n",
    "        train_images.append(img)\n",
    "        train_annotations.extend(image_id_to_annotations[img[\"id\"]])\n",
    "\n",
    "for video_name in val_video_names:\n",
    "    for img in video_to_images[video_name]:\n",
    "        val_images.append(img)\n",
    "        val_annotations.extend(image_id_to_annotations[img[\"id\"]])\n",
    "\n",
    "# Get categories from one file\n",
    "with open(all_coco_paths[0], \"r\") as f:\n",
    "    categories = json.load(f)[\"categories\"]\n",
    "\n",
    "# Save results\n",
    "with open(output_train_path, \"w\") as f:\n",
    "    json.dump({\"images\": train_images, \"annotations\": train_annotations, \"categories\": categories}, f)\n",
    "\n",
    "with open(output_val_path, \"w\") as f:\n",
    "    json.dump({\"images\": val_images, \"annotations\": val_annotations, \"categories\": categories}, f)\n",
    "\n",
    "print(f\"Train annotations saved to: {output_train_path}\")\n",
    "print(f\"Val annotations saved to: {output_val_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995eb009-a64c-47ed-8298-4c11a2621b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "v1 = \"/shared/vision/dataset/metadata/v7_8_cls/coco_vid/trimmed1000_64-500seq_test_coco_vid_06_06.json\"\n",
    "v2 = \"/shared/vision/dataset/metadata/ovis_v7/trimmed1000_fixedlen_02_26_test_split_video_sequences.json\"\n",
    "v1_data = json.load(open(v1, \"r\"))\n",
    "v2_data = json.load(open(v2, \"r\"))\n",
    "\n",
    "v1_files = []\n",
    "for vid in v1_data[\"videos\"]:\n",
    "    v1_files.extend(vid[\"file_names\"])\n",
    "    \n",
    "v2_files = []\n",
    "for vid in v2_data[\"videos\"]:\n",
    "    v2_files.extend(vid[\"file_names\"])\n",
    "    \n",
    "v1_not_in_v2 = [file for file in v1_files if file not in v2]\n",
    "v2_not_in_v1 = [file for file in v2_files if file not in v1]\n",
    "\n",
    "print(len(v1_not_in_v2))\n",
    "print(len(v2_not_in_v1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
